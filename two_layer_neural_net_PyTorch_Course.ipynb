{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PyTorch-Course-ZeweiChu-two_layer_neural_net.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1X_TKnq4nJaL"
      },
      "source": [
        "1. two_layer_neural_net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zWWEA9o1180c"
      },
      "source": [
        "from __future__ import print_function\n",
        "import torch"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTQiH60K2GGJ",
        "outputId": "54977fed-00bb-4ecf-cfa5-465fdaa1aad7"
      },
      "source": [
        "x = torch.empty(5,3)\n",
        "print(x)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[1.0081e-37, 3.0712e-41, 2.3694e-38],\n",
            "        [9.2196e-41, 1.1578e+27, 1.1362e+30],\n",
            "        [7.1547e+22, 4.5828e+30, 1.2121e+04],\n",
            "        [7.1846e+22, 9.2198e-39, 7.0374e+22],\n",
            "        [2.0474e-38, 0.0000e+00, 0.0000e+00]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mye_bksT2NIr",
        "outputId": "c42442e3-7637-4b25-cc3f-1eaa45a94051"
      },
      "source": [
        "x = torch.rand(5,3)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.3243, 0.0489, 0.5055],\n",
              "        [0.8653, 0.2405, 0.8032],\n",
              "        [0.8530, 0.4088, 0.9892],\n",
              "        [0.1252, 0.3586, 0.2395],\n",
              "        [0.1731, 0.4854, 0.5309]])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7AMIISc2Sef",
        "outputId": "6dcdb8e6-cbe1-4c63-90eb-3a40c327ad33"
      },
      "source": [
        "x = torch.zeros(5, 3, dtype=torch.long)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0],\n",
              "        [0, 0, 0]])"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOZZQF9p2X-a",
        "outputId": "c4811d86-04fe-42db-cb3d-8ce7d9754992"
      },
      "source": [
        "x = torch.tensor([5.5, 3])\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([5.5000, 3.0000])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8hkLmnmU2rJW",
        "outputId": "3542514d-83ed-4177-df0f-c8c89b004833"
      },
      "source": [
        "x = x.new_ones(5, 3, dtype=torch.double)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.],\n",
              "        [1., 1., 1.]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jl7aN3d2xT0",
        "outputId": "a2aada37-cdab-4477-d7e6-1d1464ec3cde"
      },
      "source": [
        "x = torch.randn_like(x, dtype=torch.float64)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-0.5515, -0.8704, -0.9437],\n",
              "        [-1.2185, -2.1508, -0.9661],\n",
              "        [ 0.9058,  1.4730, -1.2416],\n",
              "        [-0.2998,  1.0787,  0.2308],\n",
              "        [-1.5377, -0.1582,  0.3396]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Pbe1Dev24il",
        "outputId": "659b12cd-cd5a-44a7-9ab7-d2b9c5d3e9a0"
      },
      "source": [
        "x.size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([5, 3])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hTUc3bKB27DR",
        "outputId": "df9a587d-491d-44e5-8fcd-7c6b00d65d6b"
      },
      "source": [
        "y = torch.rand(5, 3)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.5726, 0.2521, 0.7942],\n",
              "        [0.8552, 0.0592, 0.8101],\n",
              "        [0.6558, 0.4032, 0.0982],\n",
              "        [0.8008, 0.9397, 0.6803],\n",
              "        [0.8778, 0.9382, 0.5820]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "liykIW3V3e07",
        "outputId": "fea57ed5-5c2d-47b2-ccee-ea6a78787ab8"
      },
      "source": [
        "x + y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0212, -0.6183, -0.1494],\n",
              "        [-0.3634, -2.0915, -0.1559],\n",
              "        [ 1.5616,  1.8762, -1.1434],\n",
              "        [ 0.5010,  2.0184,  0.9112],\n",
              "        [-0.6598,  0.7800,  0.9216]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzo6esa33fdi",
        "outputId": "6b065764-3c6b-4248-c7fb-11fbe7512d6b"
      },
      "source": [
        "torch.add(x, y)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0212, -0.6183, -0.1494],\n",
              "        [-0.3634, -2.0915, -0.1559],\n",
              "        [ 1.5616,  1.8762, -1.1434],\n",
              "        [ 0.5010,  2.0184,  0.9112],\n",
              "        [-0.6598,  0.7800,  0.9216]], dtype=torch.float64)"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyxCjCHn3jVu",
        "outputId": "39cef644-73da-4294-ac78-71b212134e79"
      },
      "source": [
        "y.add_(x)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[ 0.0212, -0.6183, -0.1494],\n",
              "        [-0.3634, -2.0915, -0.1559],\n",
              "        [ 1.5616,  1.8762, -1.1434],\n",
              "        [ 0.5010,  2.0184,  0.9112],\n",
              "        [-0.6598,  0.7800,  0.9216]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MXvaTb9M3rnQ",
        "outputId": "5149267d-9bc8-4c0c-a48e-e9ef38b0eba3"
      },
      "source": [
        "x = torch.rand(4,4)\n",
        "x"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9595, 0.2588, 0.3359, 0.2219],\n",
              "        [0.0817, 0.6863, 0.7256, 0.7820],\n",
              "        [0.6722, 0.3059, 0.1123, 0.2775],\n",
              "        [0.7257, 0.1293, 0.1969, 0.6495]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFQJ2zrW3zAF",
        "outputId": "75657f7f-fa10-4267-c8be-a3332f4a6c08"
      },
      "source": [
        "y = x.view(16)\n",
        "y"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.9595, 0.2588, 0.3359, 0.2219, 0.0817, 0.6863, 0.7256, 0.7820, 0.6722,\n",
              "        0.3059, 0.1123, 0.2775, 0.7257, 0.1293, 0.1969, 0.6495])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s9KcWskj31TF",
        "outputId": "0efc5281-7b9e-46fb-cef0-1659d7fec768"
      },
      "source": [
        "z = x.view(-1, 8)\n",
        "z"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.9595, 0.2588, 0.3359, 0.2219, 0.0817, 0.6863, 0.7256, 0.7820],\n",
              "        [0.6722, 0.3059, 0.1123, 0.2775, 0.7257, 0.1293, 0.1969, 0.6495]])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wdu7oO137TN",
        "outputId": "5b3156d5-6e81-4ed2-b38b-5ad03c397850"
      },
      "source": [
        "x = torch.randn(2)\n",
        "print(x)\n",
        "print(x[0].item())\n",
        "print(x.numpy())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([-0.1474, -0.9406])\n",
            "-0.14738886058330536\n",
            "[-0.14738886 -0.9406434 ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nonOTN2K3_Yy",
        "outputId": "c9f08ec2-3fd2-4672-ba18-931b96e49bec"
      },
      "source": [
        "# let us run this cell only if CUDA is available\n",
        "# We will use ``torch.device`` objects to move tensors in and out of GPU\n",
        "print(torch.cuda.is_available())\n",
        "if torch.cuda.is_available():\n",
        "    device = torch.device(\"cuda\")          # a CUDA device object\n",
        "    y = torch.ones_like(x, device=device)  # directly create a tensor on GPU\n",
        "    x = x.to(device)                       # or just use strings ``.to(\"cuda\")``\n",
        "    z = x + y\n",
        "    print(z)\n",
        "    print(z.to(\"cpu\", torch.double))       # ``.to`` can also change dtype together!"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "False\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UVe7n9SJ4zFX"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "x = np.random.randn(N, D_in)\n",
        "y = np.random.randn(N, D_out)\n",
        "\n",
        "w1 = np.random.randn(D_in, H)\n",
        "w2 = np.random.randn(H, D_out)\n",
        "\n",
        "learning_rate = 1e-6\n",
        "\n",
        "for t in range(500):\n",
        "  h = x.dot(w1)\n",
        "  h_relu = np.maximum(h, 0)\n",
        "  y_pred = h_relu.dot(w2)\n",
        "\n",
        "  loss = np.square(y_pred - y).sum()\n",
        "  print(f't {t}, loss {loss}')\n",
        "\n",
        "  # Backprop to compute gradients of w1 and w2 with respect to loss\n",
        "    \n",
        "  # loss = (y_pred - y) ** 2\n",
        "  grad_y_pred = 2.0 * (y_pred - y)\n",
        "  # \n",
        "  grad_w2 = h_relu.T.dot(grad_y_pred)\n",
        "  grad_h_relu = grad_y_pred.dot(w2.T)\n",
        "  grad_h = grad_h_relu.copy()\n",
        "  grad_h[h < 0] = 0\n",
        "  grad_w1 = x.T.dot(grad_h)\n",
        "\n",
        "  # Update weights\n",
        "  w1 -= learning_rate * grad_w1\n",
        "  w2 -= learning_rate * grad_w2"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DOWO2sLH5_hx",
        "outputId": "c3029f31-ef5d-4050-c250-755cd359c3fc"
      },
      "source": [
        "import torch\n",
        "\n",
        "\n",
        "class TwoLayerNet(torch.nn.Module):\n",
        "    def __init__(self, D_in, H, D_out):\n",
        "        \"\"\"\n",
        "        In the constructor we instantiate two nn.Linear modules and assign them as\n",
        "        member variables.\n",
        "        \"\"\"\n",
        "        super(TwoLayerNet, self).__init__()\n",
        "        self.linear1 = torch.nn.Linear(D_in, H)\n",
        "        self.linear2 = torch.nn.Linear(H, D_out)\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        In the forward function we accept a Tensor of input data and we must return\n",
        "        a Tensor of output data. We can use Modules defined in the constructor as\n",
        "        well as arbitrary operators on Tensors.\n",
        "        \"\"\"\n",
        "        h_relu = self.linear1(x).clamp(min=0)\n",
        "        y_pred = self.linear2(h_relu)\n",
        "        return y_pred\n",
        "\n",
        "\n",
        "# N is batch size; D_in is input dimension;\n",
        "# H is hidden dimension; D_out is output dimension.\n",
        "N, D_in, H, D_out = 64, 1000, 100, 10\n",
        "\n",
        "# Create random Tensors to hold inputs and outputs\n",
        "x = torch.randn(N, D_in)\n",
        "y = torch.randn(N, D_out)\n",
        "\n",
        "# Construct our model by instantiating the class defined above\n",
        "model = TwoLayerNet(D_in, H, D_out)\n",
        "\n",
        "# Construct our loss function and an Optimizer. The call to model.parameters()\n",
        "# in the SGD constructor will contain the learnable parameters of the two\n",
        "# nn.Linear modules which are members of the model.\n",
        "criterion = torch.nn.MSELoss(reduction='sum')\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
        "for t in range(500):\n",
        "    # Forward pass: Compute predicted y by passing x to the model\n",
        "    y_pred = model(x)\n",
        "\n",
        "    # Compute and print loss\n",
        "    loss = criterion(y_pred, y)\n",
        "    print(t, loss.item())\n",
        "\n",
        "    # Zero gradients, perform a backward pass, and update the weights.\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0 654.0119018554688\n",
            "1 605.621826171875\n",
            "2 563.7447509765625\n",
            "3 526.8578491210938\n",
            "4 493.9427490234375\n",
            "5 464.1705322265625\n",
            "6 437.1316223144531\n",
            "7 412.3821716308594\n",
            "8 389.62982177734375\n",
            "9 368.568115234375\n",
            "10 348.8327941894531\n",
            "11 330.46356201171875\n",
            "12 313.23577880859375\n",
            "13 296.88067626953125\n",
            "14 281.3717956542969\n",
            "15 266.6604919433594\n",
            "16 252.79090881347656\n",
            "17 239.5773162841797\n",
            "18 226.99203491210938\n",
            "19 214.96719360351562\n",
            "20 203.4740447998047\n",
            "21 192.4944610595703\n",
            "22 182.06686401367188\n",
            "23 172.1697998046875\n",
            "24 162.7066650390625\n",
            "25 153.6932830810547\n",
            "26 145.09799194335938\n",
            "27 136.92520141601562\n",
            "28 129.15115356445312\n",
            "29 121.77545166015625\n",
            "30 114.77500915527344\n",
            "31 108.13796997070312\n",
            "32 101.85194396972656\n",
            "33 95.92078399658203\n",
            "34 90.32894897460938\n",
            "35 85.0599365234375\n",
            "36 80.09194946289062\n",
            "37 75.40869903564453\n",
            "38 70.99629974365234\n",
            "39 66.84492492675781\n",
            "40 62.93670654296875\n",
            "41 59.25718688964844\n",
            "42 55.79793930053711\n",
            "43 52.549442291259766\n",
            "44 49.493629455566406\n",
            "45 46.62544631958008\n",
            "46 43.93473434448242\n",
            "47 41.408206939697266\n",
            "48 39.03710174560547\n",
            "49 36.80643844604492\n",
            "50 34.70708084106445\n",
            "51 32.73429870605469\n",
            "52 30.881505966186523\n",
            "53 29.118473052978516\n",
            "54 27.434288024902344\n",
            "55 25.852266311645508\n",
            "56 24.36646842956543\n",
            "57 22.972145080566406\n",
            "58 21.66849136352539\n",
            "59 20.448312759399414\n",
            "60 19.304523468017578\n",
            "61 18.228940963745117\n",
            "62 17.218477249145508\n",
            "63 16.268875122070312\n",
            "64 15.377662658691406\n",
            "65 14.53950023651123\n",
            "66 13.751315116882324\n",
            "67 13.009032249450684\n",
            "68 12.31119155883789\n",
            "69 11.65466022491455\n",
            "70 11.036430358886719\n",
            "71 10.454419136047363\n",
            "72 9.90630054473877\n",
            "73 9.38895320892334\n",
            "74 8.901825904846191\n",
            "75 8.438905715942383\n",
            "76 8.002345085144043\n",
            "77 7.590904235839844\n",
            "78 7.201981067657471\n",
            "79 6.834692001342773\n",
            "80 6.487725734710693\n",
            "81 6.1602277755737305\n",
            "82 5.8509063720703125\n",
            "83 5.558478832244873\n",
            "84 5.281924247741699\n",
            "85 5.02066707611084\n",
            "86 4.773249626159668\n",
            "87 4.539116382598877\n",
            "88 4.317403793334961\n",
            "89 4.107375621795654\n",
            "90 3.9082694053649902\n",
            "91 3.7196998596191406\n",
            "92 3.5411407947540283\n",
            "93 3.371727228164673\n",
            "94 3.2111282348632812\n",
            "95 3.058713674545288\n",
            "96 2.913853406906128\n",
            "97 2.7764954566955566\n",
            "98 2.6462929248809814\n",
            "99 2.522367238998413\n",
            "100 2.404623508453369\n",
            "101 2.292747974395752\n",
            "102 2.1864070892333984\n",
            "103 2.08530855178833\n",
            "104 1.9893189668655396\n",
            "105 1.8977961540222168\n",
            "106 1.810752511024475\n",
            "107 1.7279366254806519\n",
            "108 1.6491763591766357\n",
            "109 1.5741013288497925\n",
            "110 1.502706527709961\n",
            "111 1.4347118139266968\n",
            "112 1.3699833154678345\n",
            "113 1.3083943128585815\n",
            "114 1.2496616840362549\n",
            "115 1.1936699151992798\n",
            "116 1.1403316259384155\n",
            "117 1.0895359516143799\n",
            "118 1.0411044359207153\n",
            "119 0.9948952198028564\n",
            "120 0.9509430527687073\n",
            "121 0.9090539813041687\n",
            "122 0.8691486120223999\n",
            "123 0.8310549855232239\n",
            "124 0.7946568131446838\n",
            "125 0.7599502205848694\n",
            "126 0.7268332242965698\n",
            "127 0.6952051520347595\n",
            "128 0.6650121212005615\n",
            "129 0.6362065672874451\n",
            "130 0.6086922287940979\n",
            "131 0.5824235081672668\n",
            "132 0.5573375821113586\n",
            "133 0.5333725214004517\n",
            "134 0.5104772448539734\n",
            "135 0.4886454939842224\n",
            "136 0.46780669689178467\n",
            "137 0.44789284467697144\n",
            "138 0.42886924743652344\n",
            "139 0.4106975197792053\n",
            "140 0.39334356784820557\n",
            "141 0.3767663240432739\n",
            "142 0.3609301745891571\n",
            "143 0.3458009362220764\n",
            "144 0.3313239514827728\n",
            "145 0.31748083233833313\n",
            "146 0.30423176288604736\n",
            "147 0.2915770709514618\n",
            "148 0.2794773280620575\n",
            "149 0.26788759231567383\n",
            "150 0.2568003237247467\n",
            "151 0.2461872398853302\n",
            "152 0.2360418140888214\n",
            "153 0.22628673911094666\n",
            "154 0.2169472575187683\n",
            "155 0.20801332592964172\n",
            "156 0.19945687055587769\n",
            "157 0.19127047061920166\n",
            "158 0.1834423542022705\n",
            "159 0.1759500950574875\n",
            "160 0.16877281665802002\n",
            "161 0.16189686954021454\n",
            "162 0.1553148478269577\n",
            "163 0.14901554584503174\n",
            "164 0.1429835706949234\n",
            "165 0.13721084594726562\n",
            "166 0.13167713582515717\n",
            "167 0.12637846171855927\n",
            "168 0.12130283564329147\n",
            "169 0.1164434403181076\n",
            "170 0.11178530752658844\n",
            "171 0.10732419043779373\n",
            "172 0.10304863750934601\n",
            "173 0.09894973039627075\n",
            "174 0.0950193852186203\n",
            "175 0.09126055240631104\n",
            "176 0.08766406029462814\n",
            "177 0.08421638607978821\n",
            "178 0.08090633898973465\n",
            "179 0.07773423194885254\n",
            "180 0.07469972223043442\n",
            "181 0.07178081572055817\n",
            "182 0.06898635625839233\n",
            "183 0.06630219519138336\n",
            "184 0.063728928565979\n",
            "185 0.06126340478658676\n",
            "186 0.058897096663713455\n",
            "187 0.05662759765982628\n",
            "188 0.05444983392953873\n",
            "189 0.05236010625958443\n",
            "190 0.05035360902547836\n",
            "191 0.04842815175652504\n",
            "192 0.0465795062482357\n",
            "193 0.04480552300810814\n",
            "194 0.043104104697704315\n",
            "195 0.041469182819128036\n",
            "196 0.03989982604980469\n",
            "197 0.03839251771569252\n",
            "198 0.03694557398557663\n",
            "199 0.03555651754140854\n",
            "200 0.0342215932905674\n",
            "201 0.03293919563293457\n",
            "202 0.03170695900917053\n",
            "203 0.030522674322128296\n",
            "204 0.02938513457775116\n",
            "205 0.028292883187532425\n",
            "206 0.027242518961429596\n",
            "207 0.026233000680804253\n",
            "208 0.02526295930147171\n",
            "209 0.024330928921699524\n",
            "210 0.0234350748360157\n",
            "211 0.022573623806238174\n",
            "212 0.02174478769302368\n",
            "213 0.020948365330696106\n",
            "214 0.020182186737656593\n",
            "215 0.01944556087255478\n",
            "216 0.018737709149718285\n",
            "217 0.018056446686387062\n",
            "218 0.017401190474629402\n",
            "219 0.016770608723163605\n",
            "220 0.016163919121026993\n",
            "221 0.015580370090901852\n",
            "222 0.015020167455077171\n",
            "223 0.014480503275990486\n",
            "224 0.013960514217615128\n",
            "225 0.01346027385443449\n",
            "226 0.012978647835552692\n",
            "227 0.012515478767454624\n",
            "228 0.012069519609212875\n",
            "229 0.011639818549156189\n",
            "230 0.01122633833438158\n",
            "231 0.010828254744410515\n",
            "232 0.010444999672472477\n",
            "233 0.010076123289763927\n",
            "234 0.009720634669065475\n",
            "235 0.009378385730087757\n",
            "236 0.009048772975802422\n",
            "237 0.008731169626116753\n",
            "238 0.00842517614364624\n",
            "239 0.008130403235554695\n",
            "240 0.007846241816878319\n",
            "241 0.007572471164166927\n",
            "242 0.007308737374842167\n",
            "243 0.007054610643535852\n",
            "244 0.006810005288571119\n",
            "245 0.006574050057679415\n",
            "246 0.006346630398184061\n",
            "247 0.006127389147877693\n",
            "248 0.005916047841310501\n",
            "249 0.005712377373129129\n",
            "250 0.005516041535884142\n",
            "251 0.0053267162293195724\n",
            "252 0.005144196562469006\n",
            "253 0.004968177992850542\n",
            "254 0.004798445850610733\n",
            "255 0.0046347351744771\n",
            "256 0.0044769407249987125\n",
            "257 0.00432473374530673\n",
            "258 0.004177964758127928\n",
            "259 0.004036296159029007\n",
            "260 0.0038997367955744267\n",
            "261 0.003767934860661626\n",
            "262 0.0036407620646059513\n",
            "263 0.003518070559948683\n",
            "264 0.003399682929739356\n",
            "265 0.003285460639744997\n",
            "266 0.0031752577051520348\n",
            "267 0.0030688983388245106\n",
            "268 0.002966254251077771\n",
            "269 0.0028671768959611654\n",
            "270 0.002771466039121151\n",
            "271 0.0026791493874043226\n",
            "272 0.0025902825873345137\n",
            "273 0.002504214644432068\n",
            "274 0.0024211688432842493\n",
            "275 0.002340978477150202\n",
            "276 0.0022635001223534346\n",
            "277 0.0021887077018618584\n",
            "278 0.0021164738573133945\n",
            "279 0.0020467715803533792\n",
            "280 0.0019794455729424953\n",
            "281 0.0019144028192386031\n",
            "282 0.001851569744758308\n",
            "283 0.001790907816030085\n",
            "284 0.0017322978237643838\n",
            "285 0.0016756568802520633\n",
            "286 0.001620956463739276\n",
            "287 0.0015681005315855145\n",
            "288 0.0015170203987509012\n",
            "289 0.0014677041908726096\n",
            "290 0.0014200365403667092\n",
            "291 0.0013739758869633079\n",
            "292 0.001329462043941021\n",
            "293 0.0012864286545664072\n",
            "294 0.0012448441702872515\n",
            "295 0.0012046543415635824\n",
            "296 0.0011658064322546124\n",
            "297 0.0011282606283202767\n",
            "298 0.0010919789783656597\n",
            "299 0.0010569305159151554\n",
            "300 0.0010229962645098567\n",
            "301 0.0009901926387101412\n",
            "302 0.0009584826184436679\n",
            "303 0.0009278286015614867\n",
            "304 0.0008982006693258882\n",
            "305 0.0008695447468198836\n",
            "306 0.0008418389479629695\n",
            "307 0.0008150397334247828\n",
            "308 0.0007891191635280848\n",
            "309 0.0007640272378921509\n",
            "310 0.0007397765293717384\n",
            "311 0.0007163234404288232\n",
            "312 0.0006936386344023049\n",
            "313 0.0006716884090565145\n",
            "314 0.0006504684570245445\n",
            "315 0.0006299474043771625\n",
            "316 0.0006100810714997351\n",
            "317 0.0005908643361181021\n",
            "318 0.0005722708883695304\n",
            "319 0.0005542913568206131\n",
            "320 0.0005368806887418032\n",
            "321 0.0005200239247642457\n",
            "322 0.0005037375958636403\n",
            "323 0.00048796902410686016\n",
            "324 0.00047270150389522314\n",
            "325 0.0004579333763103932\n",
            "326 0.00044364389032125473\n",
            "327 0.00042981296428479254\n",
            "328 0.00041641731513664126\n",
            "329 0.0004034570592921227\n",
            "330 0.0003909202932845801\n",
            "331 0.0003787762252613902\n",
            "332 0.00036704621743410826\n",
            "333 0.00035565695725381374\n",
            "334 0.00034464371856302023\n",
            "335 0.00033397460356354713\n",
            "336 0.0003236571792513132\n",
            "337 0.0003136580344289541\n",
            "338 0.00030397155205719173\n",
            "339 0.00029459487996064126\n",
            "340 0.00028551401919685304\n",
            "341 0.00027673030854202807\n",
            "342 0.00026821630308404565\n",
            "343 0.0002599705767352134\n",
            "344 0.00025198908406309783\n",
            "345 0.00024425325682386756\n",
            "346 0.00023677467834204435\n",
            "347 0.00022952814470045269\n",
            "348 0.00022250071924645454\n",
            "349 0.0002157011185772717\n",
            "350 0.00020910779130645096\n",
            "351 0.00020272085384931415\n",
            "352 0.00019654199422802776\n",
            "353 0.0001905489625642076\n",
            "354 0.0001847496023401618\n",
            "355 0.0001791259419405833\n",
            "356 0.0001736829144647345\n",
            "357 0.00016840618627611548\n",
            "358 0.00016329110076185316\n",
            "359 0.0001583427656441927\n",
            "360 0.00015354635252151638\n",
            "361 0.00014889688463881612\n",
            "362 0.00014438919606618583\n",
            "363 0.00014002167154103518\n",
            "364 0.0001357899745926261\n",
            "365 0.00013169147132430226\n",
            "366 0.00012771552428603172\n",
            "367 0.00012386665912345052\n",
            "368 0.00012013000377919525\n",
            "369 0.00011652054672595114\n",
            "370 0.00011301047197775915\n",
            "371 0.00010961436782963574\n",
            "372 0.00010631774784997106\n",
            "373 0.00010312737867934629\n",
            "374 0.00010002937051467597\n",
            "375 9.703517571324483e-05\n",
            "376 9.412757935933769e-05\n",
            "377 9.130909165833145e-05\n",
            "378 8.857680950313807e-05\n",
            "379 8.592634549131617e-05\n",
            "380 8.33607919048518e-05\n",
            "381 8.087237802101299e-05\n",
            "382 7.8455894254148e-05\n",
            "383 7.611846376676112e-05\n",
            "384 7.3847288149409e-05\n",
            "385 7.164986163843423e-05\n",
            "386 6.951741670491174e-05\n",
            "387 6.744964775862172e-05\n",
            "388 6.544399366248399e-05\n",
            "389 6.35001779301092e-05\n",
            "390 6.161263445392251e-05\n",
            "391 5.978606714052148e-05\n",
            "392 5.801066436106339e-05\n",
            "393 5.629438965115696e-05\n",
            "394 5.462477565743029e-05\n",
            "395 5.301189958117902e-05\n",
            "396 5.1440791139611974e-05\n",
            "397 4.9921669415198267e-05\n",
            "398 4.8445446736877784e-05\n",
            "399 4.7014815208967775e-05\n",
            "400 4.562686808640137e-05\n",
            "401 4.4281470763962716e-05\n",
            "402 4.297837585909292e-05\n",
            "403 4.170987813267857e-05\n",
            "404 4.0482223994331434e-05\n",
            "405 3.929223748855293e-05\n",
            "406 3.813775037997402e-05\n",
            "407 3.7015601265011355e-05\n",
            "408 3.592934808693826e-05\n",
            "409 3.487481808406301e-05\n",
            "410 3.385097807040438e-05\n",
            "411 3.285888669779524e-05\n",
            "412 3.1894061976345256e-05\n",
            "413 3.096133877988905e-05\n",
            "414 3.005376311193686e-05\n",
            "415 2.917367237387225e-05\n",
            "416 2.831911842804402e-05\n",
            "417 2.749401937762741e-05\n",
            "418 2.668941488082055e-05\n",
            "419 2.5909313990268856e-05\n",
            "420 2.5153030946967192e-05\n",
            "421 2.442144977976568e-05\n",
            "422 2.370656693528872e-05\n",
            "423 2.301592758158222e-05\n",
            "424 2.2344021999742836e-05\n",
            "425 2.169710751331877e-05\n",
            "426 2.1064217435196042e-05\n",
            "427 2.0452642274904065e-05\n",
            "428 1.985749986488372e-05\n",
            "429 1.9280341803096235e-05\n",
            "430 1.8721962987910956e-05\n",
            "431 1.81764717126498e-05\n",
            "432 1.7651089365244843e-05\n",
            "433 1.7140089767053723e-05\n",
            "434 1.6641684851492755e-05\n",
            "435 1.616028748685494e-05\n",
            "436 1.5692410670453683e-05\n",
            "437 1.5237801562761888e-05\n",
            "438 1.4797649782849476e-05\n",
            "439 1.4369105883815791e-05\n",
            "440 1.3954808309790678e-05\n",
            "441 1.3550106814363971e-05\n",
            "442 1.315884310315596e-05\n",
            "443 1.277880983252544e-05\n",
            "444 1.2409089322318323e-05\n",
            "445 1.2051448720740154e-05\n",
            "446 1.1704630196618382e-05\n",
            "447 1.1367975275788922e-05\n",
            "448 1.1039608580176719e-05\n",
            "449 1.0721885701059364e-05\n",
            "450 1.0413711606815923e-05\n",
            "451 1.0114280485140625e-05\n",
            "452 9.82289384410251e-06\n",
            "453 9.539317034068517e-06\n",
            "454 9.26666962186573e-06\n",
            "455 9.000515092338901e-06\n",
            "456 8.742508725845255e-06\n",
            "457 8.490385880577378e-06\n",
            "458 8.24762446427485e-06\n",
            "459 8.011246791284066e-06\n",
            "460 7.781348358548712e-06\n",
            "461 7.559347977803554e-06\n",
            "462 7.341829132201383e-06\n",
            "463 7.1305139499600045e-06\n",
            "464 6.928078619239386e-06\n",
            "465 6.730000222887611e-06\n",
            "466 6.537301487696823e-06\n",
            "467 6.351698175421916e-06\n",
            "468 6.168771960801678e-06\n",
            "469 5.992954356770497e-06\n",
            "470 5.822376806463581e-06\n",
            "471 5.65670643482008e-06\n",
            "472 5.495005098055117e-06\n",
            "473 5.33769843968912e-06\n",
            "474 5.184566816751612e-06\n",
            "475 5.038001290813554e-06\n",
            "476 4.893758159596473e-06\n",
            "477 4.754397195938509e-06\n",
            "478 4.6194077185646165e-06\n",
            "479 4.48845321443514e-06\n",
            "480 4.359988906799117e-06\n",
            "481 4.2358988139312714e-06\n",
            "482 4.116174295631936e-06\n",
            "483 3.998821739514824e-06\n",
            "484 3.885126261593541e-06\n",
            "485 3.7744816836493555e-06\n",
            "486 3.6675760384241585e-06\n",
            "487 3.5635780477605294e-06\n",
            "488 3.4624995350895915e-06\n",
            "489 3.3638054901530268e-06\n",
            "490 3.2686771191947628e-06\n",
            "491 3.176828840878443e-06\n",
            "492 3.086962806264637e-06\n",
            "493 2.9989232643856667e-06\n",
            "494 2.914461674663471e-06\n",
            "495 2.8318797831161646e-06\n",
            "496 2.752027967289905e-06\n",
            "497 2.6739833174360683e-06\n",
            "498 2.5988883862737566e-06\n",
            "499 2.524566525607952e-06\n"
          ]
        }
      ]
    }
  ]
}