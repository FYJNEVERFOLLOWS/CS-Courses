{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "word-embedding_PyTorch_Course.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmCQOgSRnOk-"
      },
      "source": [
        "2. word-embedding"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xJhGu0ssYpE4"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as tud\n",
        "from torch.nn.parameter import Parameter\n",
        "\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import random\n",
        "import math\n",
        "\n",
        "import pandas as pd\n",
        "import scipy\n",
        "import sklearn\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "# 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值\n",
        "random.seed(53113)\n",
        "np.random.seed(53113)\n",
        "torch.manual_seed(53113)\n",
        "if USE_CUDA:\n",
        "    torch.cuda.manual_seed(53113)\n",
        "    \n",
        "# 设定一些超参数\n",
        "    \n",
        "K = 100 # number of negative samples\n",
        "C = 3 # nearby words threshold\n",
        "NUM_EPOCHS = 2 # The number of epochs of training\n",
        "MAX_VOCAB_SIZE = 30000 # the vocabulary size\n",
        "BATCH_SIZE = 128 # the batch size\n",
        "LEARNING_RATE = 0.2 # the initial learning rate\n",
        "EMBEDDING_SIZE = 100\n",
        "       \n",
        "    \n",
        "LOG_FILE = \"word-embedding.log\"\n",
        "\n",
        "# tokenize函数，把一篇文本转化成一个个单词\n",
        "def word_tokenize(text):\n",
        "    return text.split()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m7YbYTyn5m_1",
        "outputId": "f21adeb9-28b3-4208-9ed8-e45b20b0d6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-mh8jptzYzTK",
        "outputId": "bc017b16-47c7-4d97-ff46-6a2ff3a2b56f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "with open(\"/content/drive/My Drive/text8/text8.train.txt\", \"r\") as fin:\n",
        "    text = fin.read()\n",
        "    \n",
        "text = [w for w in word_tokenize(text.lower())]\n",
        "vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE-1))\n",
        "vocab[\"<unk>\"] = len(text) - np.sum(list(vocab.values()))\n",
        "idx_to_word = [word for word in vocab.keys()] \n",
        "word_to_idx = {word:i for i, word in enumerate(idx_to_word)}\n",
        "\n",
        "word_counts = np.array([count for count in vocab.values()], dtype=np.float32)\n",
        "word_freqs = word_counts / np.sum(word_counts)\n",
        "word_freqs = word_freqs ** (3./4.)\n",
        "word_freqs = word_freqs / np.sum(word_freqs) # 用来做 negative sampling\n",
        "VOCAB_SIZE = len(idx_to_word)\n",
        "VOCAB_SIZE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "30000"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#实现Dataloader\n",
        "一个dataloader需要以下内容：\n",
        "\n",
        "* 把所有text编码成数字，然后用subsampling预处理这些文字。\n",
        "* 保存vocabulary，单词count，normalized word frequency\n",
        "* 每个iteration sample一个中心词\n",
        "* 根据当前的中心词返回context单词\n",
        "* 根据中心词sample一些negative单词\n",
        "* 返回单词的counts"
      ],
      "metadata": {
        "id": "FVUy7tSwEzTw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class WordEmbeddingDataset(tud.Dataset):\n",
        "    def __init__(self, text, word_to_idx, idx_to_word, word_freqs, word_counts):\n",
        "        ''' text: a list of words, all text from the training dataset\n",
        "            word_to_idx: the dictionary from word to idx\n",
        "            idx_to_word: idx to word mapping\n",
        "            word_freq: the frequency of each word\n",
        "            word_counts: the word counts\n",
        "        '''\n",
        "        super(WordEmbeddingDataset, self).__init__()\n",
        "        self.text_encoded = [word_to_idx.get(t, VOCAB_SIZE-1) for t in text]\n",
        "        self.text_encoded = torch.Tensor(self.text_encoded).long()\n",
        "        self.word_to_idx = word_to_idx\n",
        "        self.idx_to_word = idx_to_word\n",
        "        self.word_freqs = torch.Tensor(word_freqs)\n",
        "        self.word_counts = torch.Tensor(word_counts)\n",
        "        \n",
        "    def __len__(self):\n",
        "        ''' 返回整个数据集（所有单词）的长度\n",
        "        '''\n",
        "        return len(self.text_encoded)\n",
        "        \n",
        "    def __getitem__(self, idx):\n",
        "        ''' 这个function返回以下数据用于训练\n",
        "            - 中心词\n",
        "            - 这个单词附近的(positive)单词\n",
        "            - 随机采样的K个单词作为negative sample\n",
        "        '''\n",
        "        center_word = self.text_encoded[idx]\n",
        "        pos_indices = list(range(idx-C, idx)) + list(range(idx+1, idx+C+1))\n",
        "        pos_indices = [i%len(self.text_encoded) for i in pos_indices]\n",
        "        pos_words = self.text_encoded[pos_indices] \n",
        "        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[0], True)\n",
        "        \n",
        "        return center_word, pos_words, neg_words "
      ],
      "metadata": {
        "id": "y-kEmhx8EzGw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 创建dataset和dataloader\n",
        "dataset = WordEmbeddingDataset(text, word_to_idx, idx_to_word, word_freqs, word_counts)\n",
        "dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=4)  "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AADuGEsyEXLP",
        "outputId": "e90e5c51-e124-4993-ff47-e43adefd1d83"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "定义PyTorch模型"
      ],
      "metadata": {
        "id": "b07QIrIWFEuq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class EmbeddingModel(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        ''' 初始化输出和输出embedding\n",
        "        '''\n",
        "        super(EmbeddingModel, self).__init__()\n",
        "        self.vocab_size = vocab_size\n",
        "        self.embed_size = embed_size\n",
        "        \n",
        "        initrange = 0.5 / self.embed_size\n",
        "        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
        "        self.out_embed.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "        \n",
        "        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=False)\n",
        "        self.in_embed.weight.data.uniform_(-initrange, initrange)\n",
        "        \n",
        "        \n",
        "    def forward(self, input_labels, pos_labels, neg_labels):\n",
        "        '''\n",
        "        input_labels: 中心词, [batch_size]\n",
        "        pos_labels: 中心词周围 context window 出现过的单词 [batch_size * (window_size * 2)]\n",
        "        neg_labelss: 中心词周围没有出现过的单词，从 negative sampling 得到 [batch_size, (window_size * 2 * K)]\n",
        "        \n",
        "        return: loss, [batch_size]\n",
        "        '''\n",
        "        \n",
        "        batch_size = input_labels.size(0)\n",
        "        \n",
        "        input_embedding = self.in_embed(input_labels) # B * embed_size\n",
        "        pos_embedding = self.out_embed(pos_labels) # B * (2*C) * embed_size\n",
        "        neg_embedding = self.out_embed(neg_labels) # B * (2*C * K) * embed_size\n",
        "      \n",
        "        log_pos = torch.bmm(pos_embedding, input_embedding.unsqueeze(2)).squeeze() # B * (2*C)\n",
        "        log_neg = torch.bmm(neg_embedding, -input_embedding.unsqueeze(2)).squeeze() # B * (2*C*K)\n",
        "\n",
        "        log_pos = F.logsigmoid(log_pos).sum(1)\n",
        "        log_neg = F.logsigmoid(log_neg).sum(1) # batch_size\n",
        "       \n",
        "        loss = log_pos + log_neg\n",
        "        \n",
        "        return -loss\n",
        "    \n",
        "    def input_embeddings(self):\n",
        "        return self.in_embed.weight.data.cpu().numpy()\n",
        "        "
      ],
      "metadata": {
        "id": "rNxjsfkQFG-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 定义一个模型以及把模型移动到GPU\n",
        "model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)\n",
        "if USE_CUDA:\n",
        "    model = model.cuda()"
      ],
      "metadata": {
        "id": "KLCebmHuls0_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 下面是评估模型的代码，以及训练模型的代码\n",
        "def evaluate(filename, embedding_weights): \n",
        "    if filename.endswith(\".csv\"):\n",
        "        data = pd.read_csv(filename, sep=\",\")\n",
        "    else:\n",
        "        data = pd.read_csv(filename, sep=\"\\t\")\n",
        "    human_similarity = []\n",
        "    model_similarity = []\n",
        "    for i in data.iloc[:, 0:2].index:\n",
        "        word1, word2 = data.iloc[i, 0], data.iloc[i, 1]\n",
        "        if word1 not in word_to_idx or word2 not in word_to_idx:\n",
        "            continue\n",
        "        else:\n",
        "            word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]\n",
        "            word1_embed, word2_embed = embedding_weights[[word1_idx]], embedding_weights[[word2_idx]]\n",
        "            model_similarity.append(float(sklearn.metrics.pairwise.cosine_similarity(word1_embed, word2_embed)))\n",
        "            human_similarity.append(float(data.iloc[i, 2]))\n",
        "\n",
        "    return scipy.stats.spearmanr(human_similarity, model_similarity)# , model_similarity\n",
        "\n",
        "def find_nearest(word):\n",
        "    index = word_to_idx[word]\n",
        "    embedding = embedding_weights[index]\n",
        "    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) for e in embedding_weights])\n",
        "    return [idx_to_word[i] for i in cos_dis.argsort()[:10]]"
      ],
      "metadata": {
        "id": "qsWbU8xQlvuw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "训练模型：\n",
        "\n",
        "* 模型一般需要训练若干个epoch\n",
        "* 每个epoch我们都把所有的数据分成若干个batch\n",
        "* 把每个batch的输入和输出都包装成cuda tensor\n",
        "* forward pass，通过输入的句子预测每个单词的下一个单词\n",
        "* 用模型的预测和正确的下一个单词计算cross entropy loss\n",
        "* 清空模型当前gradient\n",
        "* backward pass\n",
        "* 更新模型参数\n",
        "* 每隔一定的iteration输出模型在当前iteration的loss，以及在验证数据集上做模型的评估\n"
      ],
      "metadata": {
        "id": "P0zEMhZxl_XG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)\n",
        "for e in range(NUM_EPOCHS):\n",
        "    for i, (input_labels, pos_labels, neg_labels) in enumerate(dataloader):\n",
        "        \n",
        "        \n",
        "        # TODO\n",
        "        input_labels = input_labels.long()\n",
        "        pos_labels = pos_labels.long()\n",
        "        neg_labels = neg_labels.long()\n",
        "        if USE_CUDA:\n",
        "            input_labels = input_labels.cuda()\n",
        "            pos_labels = pos_labels.cuda()\n",
        "            neg_labels = neg_labels.cuda()\n",
        "            \n",
        "        optimizer.zero_grad()\n",
        "        loss = model(input_labels, pos_labels, neg_labels).mean()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            with open(LOG_FILE, \"a\") as fout:\n",
        "                fout.write(\"epoch: {}, iter: {}, loss: {}\\n\".format(e, i, loss.item()))\n",
        "                print(\"epoch: {}, iter: {}, loss: {}\".format(e, i, loss.item()))\n",
        "            \n",
        "        \n",
        "        if i % 2000 == 0:\n",
        "            embedding_weights = model.input_embeddings()\n",
        "            # sim_simlex = evaluate(\"/content/drive/My Drive/text8/SimLex-999.txt\", embedding_weights)\n",
        "            # sim_men = evaluate(\"/content/drive/My Drive/text8/men.txt\", embedding_weights)\n",
        "            sim_353 = evaluate(\"/content/drive/My Drive/text8/wordsim353.csv\", embedding_weights)\n",
        "            print(\"epoch: {}, iteration: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
        "                    e, i, sim_353, find_nearest(\"monster\")))\n",
        "            # with open(LOG_FILE, \"a\") as fout:\n",
        "            #     print(\"epoch: {}, iteration: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
        "            #         e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
        "            #     fout.write(\"epoch: {}, iteration: {}, simlex-999: {}, men: {}, sim353: {}, nearest to monster: {}\\n\".format(\n",
        "            #         e, i, sim_simlex, sim_men, sim_353, find_nearest(\"monster\")))\n",
        "                \n",
        "    embedding_weights = model.input_embeddings()\n",
        "    np.save(\"embedding-{}\".format(EMBEDDING_SIZE), embedding_weights)\n",
        "    torch.save(model.state_dict(), \"embedding-{}.th\".format(EMBEDDING_SIZE))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 240
        },
        "id": "pdolnYyal1CY",
        "outputId": "6ecf0bec-d9ce-4d5b-b8b2-2a45c4413405"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e6ab70fb366d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mNUM_EPOCHS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpos_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mneg_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "L0W3Ejx4mJR5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}