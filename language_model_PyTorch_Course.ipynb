{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqs-xyz0cZmi"
      },
      "source": [
        "学习目标\n",
        "\n",
        "* 学习语言模型，以及如何训练一个语言模型\n",
        "* 学习torchtext的基本使用方法\n",
        "  * 构建 vocabulary\n",
        "  * word to inde 和 index to word\n",
        "* 学习torch.nn的一些基本模型\n",
        "  * Linear\n",
        "  * RNN\n",
        "  * LSTM\n",
        "  * GRU\n",
        "* RNN的训练技巧\n",
        "  * Gradient Clipping\n",
        "* 如何保存和读取模型\n",
        "我们会使用 torchtext 来创建vocabulary, 然后把数据读成batch的格式。请阅读README来学习torchtext。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "VWyIuQS5YoWH"
      },
      "outputs": [],
      "source": [
        "import torchtext\n",
        "from torchtext.vocab import Vectors\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "USE_CUDA = torch.cuda.is_available()\n",
        "\n",
        "# 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值\n",
        "random.seed(53113)\n",
        "np.random.seed(53113)\n",
        "torch.manual_seed(53113)\n",
        "if USE_CUDA:\n",
        "    torch.cuda.manual_seed(53113)\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "EMBEDDING_SIZE = 650\n",
        "MAX_VOCAB_SIZE = 50000"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdvF2xw5dAmj"
      },
      "source": [
        "* 我们会继续使用上次的text8作为我们的训练，验证和测试数据\n",
        "* TorchText的一个重要概念是Field，它决定了你的数据会如何被处理。我们使用TEXT这个field来处理文本数据。我们的TEXT field有lower=True这个参数，所以所有的单词都会被lowercase。\n",
        "* torchtext提供了LanguageModelingDataset这个class来帮助我们处理语言模型数据集。\n",
        "* build_vocab可以根据我们提供的训练数据集来创建最高频单词的单词表，max_size帮助我们限定单词总量。\n",
        "* BPTTIterator可以连续地得到连贯的句子，BPTT的全程是back propagation through time。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F55StYQafRUu",
        "outputId": "b54ed6c4-8bda-46a8-e572-952348fc1443"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lBM2XmxEc-Qp",
        "outputId": "82859fc2-f709-4937-86e7-eb38261717e5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n",
            "The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "vocabulary size: 50002\n"
          ]
        }
      ],
      "source": [
        "TEXT = torchtext.legacy.data.Field(lower=True)\n",
        "train, val, test = torchtext.legacy.datasets.LanguageModelingDataset.splits(path=\".\", \n",
        "    train=\"/content/drive/My Drive/text8/text8.train.txt\", validation=\"/content/drive/My Drive/text8/text8.dev.txt\", test=\"/content/drive/My Drive/text8/text8.test.txt\", text_field=TEXT)\n",
        "TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)\n",
        "print(\"vocabulary size: {}\".format(len(TEXT.vocab)))\n",
        "\n",
        "VOCAB_SIZE = len(TEXT.vocab)\n",
        "train_iter, val_iter, test_iter = torchtext.legacy.data.BPTTIterator.splits(\n",
        "    (train, val, test), batch_size=BATCH_SIZE, device=-1, bptt_len=32, repeat=False, shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvFP-HSOfk3f"
      },
      "source": [
        "* 为什么我们的单词表有50002个单词而不是50000呢？因为TorchText给我们增加了两个特殊的token，<unk>表示未知的单词，<pad>表示padding。\n",
        "* 模型的输入是一串文字，模型的输出也是一串文字，他们之间相差一个位置，因为语言模型的目标是根据之前的单词预测下一个单词。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z3LDsXSIfmrJ",
        "outputId": "f2a1842d-e5e2-4cad-aaf2-ecc07c2332d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "had dropped to just three zero zero zero k it was then cool enough to allow the nuclei to capture electrons this process is called recombination during which the first neutral atoms\n",
            "dropped to just three zero zero zero k it was then cool enough to allow the nuclei to capture electrons this process is called recombination during which the first neutral atoms took\n"
          ]
        }
      ],
      "source": [
        "it = iter(train_iter)\n",
        "batch = next(it)\n",
        "print(\" \".join([TEXT.vocab.itos[i] for i in batch.text[:,1].data]))\n",
        "print(\" \".join([TEXT.vocab.itos[i] for i in batch.target[:,1].data]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kmBiJ33CfqGl",
        "outputId": "3a24621c-c50b-48f4-c53c-2d0798de54df"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "at first contact with the japanese people was friendly and both were equals in a trade relationship however eventually the japanese started to dominate the relationship and soon established large settlements on\n",
            "first contact with the japanese people was friendly and both were equals in a trade relationship however eventually the japanese started to dominate the relationship and soon established large settlements on the\n",
            "the outskirts of ainu territory as the japanese moved north and took control over their traditional lands the ainu often gave up without resistance but there was occasional resistance as exemplified in\n",
            "outskirts of ainu territory as the japanese moved north and took control over their traditional lands the ainu often gave up without resistance but there was occasional resistance as exemplified in wars\n",
            "wars in one four five seven one six six nine and one seven eight nine all of which were lost by the ainu japanese policies became increasingly aimed at assimilating the ainu\n",
            "in one four five seven one six six nine and one seven eight nine all of which were lost by the ainu japanese policies became increasingly aimed at assimilating the ainu in\n",
            "in the meiji period outlawing their language and restricting them to farming on government provided plots ainu were also used in near slavery conditions in the japanese fishing industry the island of\n",
            "the meiji period outlawing their language and restricting them to farming on government provided plots ainu were also used in near slavery conditions in the japanese fishing industry the island of hokkaido\n",
            "hokkaido was called <unk> or <unk> chi during the edo period its name was changed to hokkaido during the meiji restoration as part of the programme to unify the japanese national character\n",
            "was called <unk> or <unk> chi during the edo period its name was changed to hokkaido during the meiji restoration as part of the programme to unify the japanese national character under\n"
          ]
        }
      ],
      "source": [
        "for i in range(5):\n",
        "    batch = next(it)\n",
        "    print(\" \".join([TEXT.vocab.itos[i] for i in batch.text[:,2].data]))\n",
        "    print(\" \".join([TEXT.vocab.itos[i] for i in batch.target[:,2].data]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_VwsQayftx7"
      },
      "source": [
        "# 定义模型\n",
        "* 继承nn.Module\n",
        "* 初始化函数\n",
        "* forward函数\n",
        "* 其余可以根据模型需要定义相关的函数"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "5WT6uwtMfqjj"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class RNNModel(nn.Module):\n",
        "    \"\"\" 一个简单的循环神经网络\"\"\"\n",
        "\n",
        "    def __init__(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=0.5):\n",
        "        ''' 该模型包含以下几层:\n",
        "            - 词嵌入层\n",
        "            - 一个循环神经网络层(RNN, LSTM, GRU)\n",
        "            - 一个线性层，从hidden state到输出单词表\n",
        "            - 一个dropout层，用来做regularization\n",
        "        '''\n",
        "        super(RNNModel, self).__init__()\n",
        "        self.drop = nn.Dropout(dropout)\n",
        "        self.encoder = nn.Embedding(ntoken, ninp)\n",
        "        if rnn_type in ['LSTM', 'GRU']:\n",
        "            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)\n",
        "        else:\n",
        "            try:\n",
        "                nonlinearity = {'RNN_TANH': 'tanh', 'RNN_RELU': 'relu'}[rnn_type]\n",
        "            except KeyError:\n",
        "                raise ValueError( \"\"\"An invalid option for `--model` was supplied,\n",
        "                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']\"\"\")\n",
        "            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)\n",
        "        self.decoder = nn.Linear(nhid, ntoken)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "        self.rnn_type = rnn_type\n",
        "        self.nhid = nhid\n",
        "        self.nlayers = nlayers\n",
        "\n",
        "    def init_weights(self):\n",
        "        initrange = 0.1\n",
        "        self.encoder.weight.data.uniform_(-initrange, initrange)\n",
        "        self.decoder.bias.data.zero_()\n",
        "        self.decoder.weight.data.uniform_(-initrange, initrange)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        ''' Forward pass:\n",
        "            - word embedding\n",
        "            - 输入循环神经网络\n",
        "            - 一个线性层从hidden state转化为输出单词表\n",
        "        '''\n",
        "        emb = self.drop(self.encoder(input))\n",
        "        output, hidden = self.rnn(emb, hidden)\n",
        "        output = self.drop(output)\n",
        "        decoded = self.decoder(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return decoded.view(output.size(0), output.size(1), decoded.size(1)), hidden\n",
        "\n",
        "    def init_hidden(self, bsz, requires_grad=True):\n",
        "        weight = next(self.parameters())\n",
        "        if self.rnn_type == 'LSTM':\n",
        "            return (weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad),\n",
        "                    weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad))\n",
        "        else:\n",
        "            return weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQgrfb0ogAnu"
      },
      "outputs": [],
      "source": [
        "# 初始化一个模型\n",
        "model = RNNModel(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
        "if USE_CUDA:\n",
        "    model = model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVY871_mgFtM"
      },
      "source": [
        "* 我们首先定义评估模型的代码。\n",
        "* 模型的评估和模型的训练逻辑基本相同，唯一的区别是我们只需要forward pass，不需要backward pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mV1B0-nggFbG"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, data):\n",
        "    model.eval()\n",
        "    total_loss = 0.\n",
        "    it = iter(data)\n",
        "    total_count = 0.\n",
        "    with torch.no_grad():\n",
        "        hidden = model.init_hidden(BATCH_SIZE, requires_grad=False)\n",
        "        for i, batch in enumerate(it):\n",
        "            data, target = batch.text, batch.target\n",
        "            if USE_CUDA:\n",
        "                data, target = data.cuda(), target.cuda()\n",
        "            hidden = repackage_hidden(hidden)\n",
        "            with torch.no_grad():\n",
        "                output, hidden = model(data, hidden)\n",
        "            loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1))\n",
        "            total_count += np.multiply(*data.size())\n",
        "            total_loss += loss.item()*np.multiply(*data.size())\n",
        "            \n",
        "    loss = total_loss / total_count\n",
        "    model.train()\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x6F7fHnKhQxI"
      },
      "outputs": [],
      "source": [
        "# 定义下面的一个function，帮助我们把一个hidden state和计算图之前的历史分离。\n",
        "def repackage_hidden(h):\n",
        "    \"\"\"Wraps hidden states in new Tensors, to detach them from their history.\"\"\"\n",
        "    if isinstance(h, torch.Tensor):\n",
        "        return h.detach()\n",
        "    else:\n",
        "        return tuple(repackage_hidden(v) for v in h)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G0wEysWVgRIm"
      },
      "source": [
        "定义 loss function 和 optimizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QCoZUtn2gQ5H"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.001\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, 0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s59t_gqQg2bb"
      },
      "source": [
        "训练模型：\n",
        "\n",
        "* 模型一般需要训练若干个epoch\n",
        "* 每个epoch我们都把所有的数据分成若干个batch\n",
        "* 把每个batch的输入和输出都包装成cuda tensor\n",
        "* forward pass，通过输入的句子预测每个单词的下一个单词\n",
        "* 用模型的预测和正确的下一个单词计算cross entropy loss\n",
        "* 清空模型当前gradient\n",
        "* backward pass\n",
        "* gradient clipping，防止梯度爆炸\n",
        "* 更新模型参数\n",
        "* 每隔一定的iteration输出模型在当前iteration的loss，以及在验证集上做模型的评估"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "myMYlHkpg990",
        "outputId": "bb7a8e71-3492-485e-8708-9b1c796195d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0 iter 0 loss 10.82181453704834\n",
            "best model, val loss:  10.778357779158656\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "GRAD_CLIP = 1.\n",
        "NUM_EPOCHS = 2\n",
        "\n",
        "val_losses = []\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    model.train()\n",
        "    it = iter(train_iter)\n",
        "    hidden = model.init_hidden(BATCH_SIZE)\n",
        "    for i, batch in enumerate(it):\n",
        "        data, target = batch.text, batch.target\n",
        "        if USE_CUDA:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "        hidden = repackage_hidden(hidden)\n",
        "        model.zero_grad()\n",
        "        output, hidden = model(data, hidden)\n",
        "        loss = loss_fn(output.view(-1, VOCAB_SIZE), target.view(-1))\n",
        "        loss.backward()\n",
        "        ### gradient clipping，防止梯度爆炸 ###\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)\n",
        "        optimizer.step()\n",
        "        if i % 1000 == 0:\n",
        "            print(\"epoch\", epoch, \"iter\", i, \"loss\", loss.item())\n",
        "    \n",
        "        if i % 10000 == 0:\n",
        "            val_loss = evaluate(model, val_iter)\n",
        "            \n",
        "            if len(val_losses) == 0 or val_loss < min(val_losses):\n",
        "                print(\"best model, val loss: \", val_loss)\n",
        "                torch.save(model.state_dict(), \"/content/drive/My Drive/text8/lm-best.th\")\n",
        "            else:\n",
        "                scheduler.step()\n",
        "                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "            val_losses.append(val_loss)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xN7Ei3W3ryCi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc313e4f-980e-4412-aae4-03c31b48f7a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "best_model = RNNModel(\"LSTM\", VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, 2, dropout=0.5)\n",
        "if USE_CUDA:\n",
        "    best_model = best_model.cuda()\n",
        "best_model.load_state_dict(torch.load(\"/content/drive/My Drive/text8/lm-best.th\"))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 使用训练好的模型生成一些句子。\n",
        "hidden = best_model.init_hidden(1)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "input = torch.randint(VOCAB_SIZE, (1, 1), dtype=torch.long).to(device)\n",
        "words = []\n",
        "for i in range(100):\n",
        "    output, hidden = best_model(input, hidden)\n",
        "    word_weights = output.squeeze().exp().cpu()\n",
        "    word_idx = torch.multinomial(word_weights, 1)[0]\n",
        "    input.fill_(word_idx)\n",
        "    word = TEXT.vocab.itos[word_idx]\n",
        "    words.append(word)\n",
        "print(\" \".join(words))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VtzLZbV6A_R5",
        "outputId": "9d10f63b-d1e4-4e55-be7e-ab515ab5e5fb"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "qianlong bangla yankee gateless briand triplet hardening idolatry exclusion agavaceae triomphe challenged benny doc accord datagrams outfit jewel convoluted nni distal ritchie duress perpendicular bert kumasi roh learner trampled overcomes kia pulley monody seleucid flavors maha susa kword hardships lans samsa shirow israel wah tighten sampa damin routledge devotees recreate eukaryotes substitutions gc rockne redeeming riddler argumentative kilobyte triadic enthusiastic moored crone gladiators trivia unchecked domains mounted formatting invertebrates necropolis conservationist coincident comprehended patriarch sadie byproduct seagull earthquake nfs applicants jaya fluctuations hark misinformation heraclea finn speciality entities hawke reside cps tethys mongoloids unionist flats lilly witwatersrand kumquat satchmo nutritious\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "language-model.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}